{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "12O538tiGvILJ1Qt6mxAgpiehp9vM0gvI",
      "authorship_tag": "ABX9TyNWji1g15/m1bA+cVTo4IGN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kidrah-git/-Data-Visualisation-/blob/main/week_7%20(assignment).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q10yCigSQ7EZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def load_data(directory, encoding='latin-1'): # Changed default encoding to 'latin-1'\n",
        "    texts = []\n",
        "    labels = []\n",
        "\n",
        "    # Loop through each file in the directory\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            label = filename.split('_')[0]  # Assuming labels are in the filename, like 'label_filename.txt'\n",
        "            # Try opening with specified encoding, fallback to 'utf-8' if it fails\n",
        "            try:\n",
        "                with open(os.path.join(directory, filename), 'r', encoding=encoding) as file:\n",
        "                    texts.append(file.read())\n",
        "                    labels.append(label)\n",
        "            except UnicodeDecodeError:\n",
        "                print(f\"Failed to decode {filename} with {encoding}, trying utf-8\")\n",
        "                try:\n",
        "                    with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
        "                        texts.append(file.read())\n",
        "                        labels.append(label)\n",
        "                except UnicodeDecodeError:\n",
        "                    print(f\"Failed to decode {filename} with utf-8 as well, skipping\")\n",
        "\n",
        "    return texts, np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "data_dir = \"/content/drive/MyDrive/Dataset (final project)\"\n",
        "texts, labels = load_data(data_dir)"
      ],
      "metadata": {
        "id": "LJUvUPpRRjwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Count Vectorization**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5i5H2YOSU31e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "\n",
        "def tokenize(text):\n",
        "    # Remove punctuation and tokenize\n",
        "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "def build_vocab(texts):\n",
        "    vocabulary = {}\n",
        "    index = 0\n",
        "    for text in texts:\n",
        "        tokens = tokenize(text)\n",
        "        for token in tokens:\n",
        "            if token not in vocabulary:\n",
        "                vocabulary[token] = index\n",
        "                index += 1\n",
        "    return vocabulary\n",
        "\n",
        "def text_to_vector(text, vocabulary):\n",
        "    vector = [0] * len(vocabulary)\n",
        "    tokens = tokenize(text)\n",
        "    for token in tokens:\n",
        "        if token in vocabulary:\n",
        "            index = vocabulary[token]\n",
        "            vector[index] += 1\n",
        "    return vector\n",
        "\n",
        "# Build vocabulary\n",
        "vocabulary = build_vocab(texts)\n",
        "\n",
        "# Convert texts to vectors\n",
        "X = np.array([text_to_vector(text, vocabulary) for text in texts])\n"
      ],
      "metadata": {
        "id": "UA9Z5V5GTZrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Label Encoding**"
      ],
      "metadata": {
        "id": "1Vy5nzanUPIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_labels(labels):\n",
        "    label_to_index = {}\n",
        "    index = 0\n",
        "    encoded_labels = []\n",
        "\n",
        "    for label in labels:\n",
        "        if label not in label_to_index:\n",
        "            label_to_index[label] = index\n",
        "            index += 1\n",
        "        encoded_labels.append(label_to_index[label])\n",
        "\n",
        "    return np.array(encoded_labels), label_to_index\n",
        "\n",
        "# Encode labels\n",
        "y, label_to_index = encode_labels(labels)\n"
      ],
      "metadata": {
        "id": "fnbfLqHHT1zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression**\n"
      ],
      "metadata": {
        "id": "RksXbyzSUCai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Initialize weights and bias\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Gradient descent\n",
        "        for _ in range(self.epochs):\n",
        "            # Linear model\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            # Apply sigmoid function\n",
        "            y_predicted = self.sigmoid(linear_model)\n",
        "\n",
        "            # Compute gradients\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            # Update weights and bias\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self.sigmoid(linear_model)\n",
        "        return [1 if i > 0.5 else 0 for i in y_predicted]\n",
        "\n",
        "# Training the model\n",
        "model = LogisticRegression(learning_rate=0.01, epochs=1000)\n",
        "model.fit(X, y)\n"
      ],
      "metadata": {
        "id": "siTrl5Y9T2yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evalauation**"
      ],
      "metadata": {
        "id": "EgstFg5fT_5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "accuracy = accuracy_score(y, y_pred)\n",
        "# Set average to 'micro', 'macro', 'weighted', or None\n",
        "precision = precision_score(y, y_pred, average='micro')\n",
        "recall = recall_score(y, y_pred, average='micro')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsLNiyAtVSoL",
        "outputId": "2e3924a7-be97-4274-8fe7-406da90d8413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.05\n",
            "Precision: 0.05\n",
            "Recall: 0.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KesU9TYgVW6P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}